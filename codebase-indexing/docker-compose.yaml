services:
  embedding-model:
    image:  yevai/codebase-index-embed:sm120-cu131-v1
    env_file:
      - .env
    profiles: ["codebase-indexing"]
    container_name: embedding-inference
    restart: "no"
    labels:
      - "ephemeral=true"
      - "persistence=none"
    runtime: nvidia
    pid: host
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_CACHE_DIR}:/data
    command:
      - --model-id
      - ${EMBEDDING_MODEL_PATH}
      - --pooling
      - last-token
      - --max-client-batch-size
      - "${MAX_CLIENT_BATCH_SIZE:-64}"
      - --max-batch-tokens
      - "${MAX_BATCH_TOKENS:-32768}"
  embedding-manager:
    image: yevai/codebase-index-rerank:sm120-cu131-v1
    env_file:
      - .env
    depends_on:
      - embedding-model
      - db-vector
    profiles: ["codebase-indexing"]
    container_name: embedding-manager
    restart: "no"
    labels:
      - "ephemeral=true"
      - "persistence=none"
    runtime: nvidia
    pid: host
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "1335:8000"
    volumes:
      - ${HF_CACHE_DIR}:/data
    environment:
      - TEI_BASE_URL=${TEI_BASE_URL}
      - MODEL_PATH=${RERANKER_MODEL_PATH}
      - PORT=8000
      - HF_HUB_OFFLINE=1
  db-vector:
    image: qdrant/qdrant:latest
    profiles: ["codebase-indexing"]
    container_name: db-vector
    ports:
      - "6333:6333"
      - "6334:6334"
    labels:
      - "ephemeral=false"
      - "persistence=hfs"
    volumes:
      - ${HOME}/.qdrant_data:/qdrant/storage